{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "530d58ea-70c8-4539-882d-0bc232c1aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730431f-91e0-44da-9006-172dbb5705ce",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e2e2a07-7c5b-46c7-a52c-b433e6916979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>favourites</th>\n",
       "      <th>statuses</th>\n",
       "      <th>retweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>637894677824413696</td>\n",
       "      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>637890384576778240</td>\n",
       "      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>637749345908051968</td>\n",
       "      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>637696421077123073</td>\n",
       "      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>637696327485366272</td>\n",
       "      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n",
       "      <td>It’s hard to say whether packing lists are mak...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              post_id                    post_created  \\\n",
       "0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
       "1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
       "2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
       "3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
       "4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
       "\n",
       "                                           post_text     user_id  followers  \\\n",
       "0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n",
       "1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n",
       "2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n",
       "3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n",
       "4  It’s hard to say whether packing lists are mak...  1013187241         84   \n",
       "\n",
       "   friends  favourites  statuses  retweets  label  \n",
       "0      211         251       837         0      1  \n",
       "1      211         251       837         1      1  \n",
       "2      211         251       837         0      1  \n",
       "3      211         251       837         2      1  \n",
       "4      211         251       837         1      1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file into a dataframe.\n",
    "df = pd.read_csv(\"depression.csv\", index_col=0)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ff3f4-6448-4db9-9ceb-fd4932957216",
   "metadata": {},
   "source": [
    "## Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc7aa8-8945-416e-8d7b-56461af3f2be",
   "metadata": {},
   "source": [
    "##### Only columns relevant for exploring the underlying themes of the tweets and how these themes correlate with depression are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4a4ec25-6e2c-4fc8-b2ed-7c35a697aea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>637894677824413696</td>\n",
       "      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>637890384576778240</td>\n",
       "      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>637749345908051968</td>\n",
       "      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>637696421077123073</td>\n",
       "      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>637696327485366272</td>\n",
       "      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n",
       "      <td>It’s hard to say whether packing lists are mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              post_id                    post_created  \\\n",
       "0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
       "1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
       "2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
       "3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
       "4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
       "\n",
       "                                           post_text  label  \n",
       "0  It's just over 2 years since I was diagnosed w...      1  \n",
       "1  It's Sunday, I need a break, so I'm planning t...      1  \n",
       "2  Awake but tired. I need to sleep but my brain ...      1  \n",
       "3  RT @SewHQ: #Retro bears make perfect gifts and...      1  \n",
       "4  It’s hard to say whether packing lists are mak...      1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnecessaru columns.\n",
    "df = df.drop(columns=['user_id', 'followers', 'friends', 'favourites', 'statuses', 'retweets'])\n",
    "# Remove the index column.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f160-025c-440a-b424-aa36c2017bfa",
   "metadata": {},
   "source": [
    "## Handle Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dac9bec1-3520-493d-948f-68ca604fded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows before removal: 119\n"
     ]
    }
   ],
   "source": [
    " print(f\"Number of duplicated rows before removal: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd478957-0010-4901-8f70-3c2891f7b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicated rows.\n",
    "df.duplicated() \n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39375123-0e7d-483d-98fe-e0aeceeedffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the duplicated rows have been removed.\n",
    "print(f\"Number of duplicated rows after removal: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac538e-5d7b-479b-a5bc-7512361292c2",
   "metadata": {},
   "source": [
    "## Convert Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d233e69-e88d-40fb-b62e-b964929eb762",
   "metadata": {},
   "source": [
    "### Convert Tweet Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8175da56-2b6a-464d-aa3f-9029491097d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column from 'post_id' to 'id'.\n",
    "df = df.rename(columns={'post_id': 'id'})\n",
    "\n",
    "# Convert the identifiers to strings.\n",
    "df['id'] = df['id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8a6a1-4f20-449a-8571-ba0f675776eb",
   "metadata": {},
   "source": [
    "### Convert Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5c25da2-bd9c-4e0c-896b-d53f169aef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column from 'post_created' to 'date'.\n",
    "df = df.rename(columns={'post_created': 'date'})\n",
    "\n",
    "# Convert the 'date' column to datetime format.\n",
    "df['date'] = pd.to_datetime(df['date'], format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "# Extract the relevant datetime components.\n",
    "df['weekday'] = df['date'].dt.strftime('%A')\n",
    "df['month'] = df['date'].dt.strftime('%B')\n",
    "df['year'] = df['date'].dt.year\n",
    "df['day'] = df['date'].dt.day\n",
    "df['hour'] = df['date'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17faaa4-b94f-4c18-ab68-2fabc4e13178",
   "metadata": {},
   "source": [
    "### Convert Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "360a202f-1020-4026-978f-c3132b497f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT @SewHQ: #Retro bears make perfect gifts and...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rename the column from 'post_text' to 'tweet'.\n",
    "df = df.rename(columns={'post_text': 'tweet'})\n",
    "\n",
    "# Convert the tweets to strings.\n",
    "df['tweet'] = df['tweet'].astype(str)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa305b07-63aa-417b-8b9f-ac5e6a2defde",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26034d17-ed05-4b49-84ce-56f89bc82f5d",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "184b266c-12fd-401d-9213-4c729f0bc21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT @SewHQ: #Retro bears make perfect gifts and...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove URLs from a tweet using regex.\n",
    "def remove_urls(tweet):\n",
    "    return re.sub(r'http\\S+', '', tweet)\n",
    "\n",
    "# Apply remove_urls to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_urls)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe04352-43fb-49d9-b0e5-91f5a4e1933a",
   "metadata": {},
   "source": [
    "### Remove Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba9e30-240e-47dd-b2b7-61db3108b189",
   "metadata": {},
   "source": [
    "##### Remove @username mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f792a8ed-6761-45fe-9fe8-d17cb9db7ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT : #Retro bears make perfect gifts and are g...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove mentions from a tweet using regex.\n",
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "# Apply remove_mentions to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_mentions)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7254f4-4dc9-4751-b45a-c159d9146056",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fab08048-7d23-4136-bcb7-c138a3d8ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Its just over 2 years since I was diagnosed wi...\n",
      "1        Its Sunday I need a break so Im planning to sp...\n",
      "2        Awake but tired I need to sleep but my brain h...\n",
      "3        RT  Retro bears make perfect gifts and are gre...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                 A day without sunshine is like night\n",
      "19996    Borens Laws 1 When in charge ponder 2 When in ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor but they were never m...\n",
      "19999        Black holes are where God is dividing by zero\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove punctuation from a tweet.\n",
    "def remove_punctuation(tweet):\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Apply remove_punctuation to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_punctuation)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b438c-c0e5-49e0-a97f-fe8091dcc9cc",
   "metadata": {},
   "source": [
    "### Remove Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a62dc257-e971-4f3a-ad6c-d6c8ed2b486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Its just over 2 years since I was diagnosed wi...\n",
      "1        Its Sunday I need a break so Im planning to sp...\n",
      "2        Awake but tired I need to sleep but my brain h...\n",
      "3        RT Retro bears make perfect gifts and are grea...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                 A day without sunshine is like night\n",
      "19996    Borens Laws 1 When in charge ponder 2 When in ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor but they were never m...\n",
      "19999        Black holes are where God is dividing by zero\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove extra spaces from a tweet with regex.\n",
    "def remove_extra_spaces(tweet):\n",
    "    return re.sub(r'\\s+', ' ', tweet).strip()\n",
    "\n",
    "# Apply remove_extra_spaces to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_extra_spaces)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9af8f-eb41-440a-924b-7dc0c0e76e7b",
   "metadata": {},
   "source": [
    "### Convert Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59686850-59bd-4a5b-ae7b-ee3d2c09152e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        its just over 2 years since i was diagnosed wi...\n",
      "1        its sunday i need a break so im planning to sp...\n",
      "2        awake but tired i need to sleep but my brain h...\n",
      "3        rt retro bears make perfect gifts and are grea...\n",
      "4        it’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                 a day without sunshine is like night\n",
      "19996    borens laws 1 when in charge ponder 2 when in ...\n",
      "19997    the flow chart is a most thoroughly oversold p...\n",
      "19998    ships are safe in harbor but they were never m...\n",
      "19999        black holes are where god is dividing by zero\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'tweets' column to lowercase.\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9beb42-a68d-40e8-9a36-197e1eb2f46a",
   "metadata": {},
   "source": [
    "### Lemmatise Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da0b2b0d-f456-4807-97b4-341279ad224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a99a003-db7d-4009-800b-85b924c60a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise lemmatiser.\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74fb23d-ae69-4fa9-88c0-6b16e476a45b",
   "metadata": {},
   "source": [
    "#### Lemmatise Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43d6eb03-5d24-4df2-b130-4ca6b361a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        it just over 2 year since i wa diagnosed with ...\n",
      "1        it sunday i need a break so im planning to spe...\n",
      "2        awake but tired i need to sleep but my brain h...\n",
      "3        rt retro bear make perfect gift and are great ...\n",
      "4        it ’ s hard to say whether packing list are ma...\n",
      "                               ...                        \n",
      "19995                 a day without sunshine is like night\n",
      "19996    borens law 1 when in charge ponder 2 when in t...\n",
      "19997    the flow chart is a most thoroughly oversold p...\n",
      "19998    ship are safe in harbor but they were never me...\n",
      "19999         black hole are where god is dividing by zero\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function for lemmatising nouns in a tweet.\n",
    "def lemmatise_nouns(tweet):\n",
    "    # Tokenise the tweet into individual words.\n",
    "    words = word_tokenize(tweet)\n",
    "    # Lemmatise each noun in the tweet.\n",
    "    lemmatised_words = [lemmatiser.lemmatize(word) for word in words]\n",
    "    # Join the lemmatised words back into a string.\n",
    "    return \" \".join(lemmatised_words)\n",
    "\n",
    "# Apply lemmatise_nouns to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(lemmatise_nouns)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e289486-98f1-4dc6-bed6-1183e2b0eb4b",
   "metadata": {},
   "source": [
    "#### Lemmatise Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041adb0e-91c3-4648-9a03-c862599b61b7",
   "metadata": {},
   "source": [
    "The WordNetLemmatizer assumes all words are nouns. To get more accurate lemmatisation, such that verbs and adjectives are also lemmatised, part-of-speech (POS) tags can be used to specify the correct word category, allowing the lemmatiser to reduce words to their base forms based on their actual usage in the sentence. For example, 'running' is lemmatised to 'run', and 'better' to 'good'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2268d446-62e4-446a-b2f8-55cc3ec797cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        it just over 2 year since i wa diagnose with a...\n",
      "1        it sunday i need a break so im planning to spe...\n",
      "2        awake but tire i need to sleep but my brain ha...\n",
      "3        rt retro bear make perfect gift and be great f...\n",
      "4        it ’ s hard to say whether pack list be make l...\n",
      "                               ...                        \n",
      "19995                 a day without sunshine be like night\n",
      "19996    borens law 1 when in charge ponder 2 when in t...\n",
      "19997    the flow chart be a most thoroughly oversold p...\n",
      "19998    ship be safe in harbor but they be never mean ...\n",
      "19999            black hole be where god be divide by zero\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function for lemmatising words in a tweet with POS.\n",
    "def lemmatise_words_with_pos(tweet):\n",
    "    # Tokenise the tweet into individual words.\n",
    "    words = word_tokenize(tweet)\n",
    "    # Get POS tags for each word.\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    # Map POS tags to WordNet POS tags.\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    # Lemmatise based on POS tags.\n",
    "    lemmatised_words = [lemmatiser.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "    # Join the lemmatised words back into a string.\n",
    "    return \" \".join(lemmatised_words)\n",
    "\n",
    "# Apply lemmatise_words_with_pos to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(lemmatise_words_with_pos)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407e585-96a1-45ba-8368-ce953238f234",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9949d1bd-b890-4946-811d-0c8b6ddbeb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        diagnose anxiety depression moment reflect far...\n",
      "1        sunday need break planning spend little possib...\n",
      "2                         awake tire need sleep brain idea\n",
      "3        retro bear perfect gift great beginner stitch ...\n",
      "4        hard whether pack list life easier reinforce n...\n",
      "                               ...                        \n",
      "19995                               without sunshine night\n",
      "19996    borens law charge ponder trouble delegate doub...\n",
      "19997    flow chart thoroughly oversold piece program d...\n",
      "19998                          ship safe harbor never mean\n",
      "19999                           black hole god divide zero\n",
      "Name: tweet, Length: 19881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the stopwords from nltk.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add additional stopwords.\n",
    "additional_stopwords = {\n",
    "    \n",
    "    # Common verbs.\n",
    "    'go', 'get', 'wait', 'may', 'find', 'say', 'thank', 'see', 'could', 'would', 'like', 'look', 'know', 'try', 'make', 'talk',\n",
    "    'stop', 'let', 'watch', 'keep', 'happen', 'take', 'wear', 'call', 'come', 'tell', 'stay', 'move', 'give', 'check',\n",
    "    \n",
    "    \n",
    "    # Common nouns.\n",
    "    'week', 'way', 'time', 'morning', 'day', 'year', 'today', 'anything', 'something', 'end', 'thing', 'mind', 'one', 'part', 'name',\n",
    "\n",
    "    # Common pronouns.\n",
    "    'he', 'she',\n",
    "\n",
    "    # Common adjectives and adverbs.\n",
    "    'much', 'still', 'new', 'back', 'real', 'anytime', 'even', 'right', 'also', 'cool', 'already', 'ever', 'sure', 'long', 'pretty', \n",
    "    'since', 'old', 'first', 'really', 'around', 'two',\n",
    "\n",
    "    # Common interjections.\n",
    "    'thanks', 'hey', 'hello', 'wow', 'oh', 'ok', 'well', 'yeah', 'actually', 'maybe', 'yes', 'no', 'please', 'haha',\n",
    "\n",
    "    # Contractions and shorthands.\n",
    "    'cant', 'via', 'thats', 'youre', 'dont', 'theyre', 'shes', 'doesnt', 'didnt', 'whats',\n",
    "    \n",
    "    # Twitter-specific words.\n",
    "    'rt', 'follow', 'twitter', 'tweet',\n",
    "    \n",
    "    # Undefined words.\n",
    "    'amp', 'lt3', 'yong', 'na', 'gon', 'wan', 'wearepayting', 'foryong', 'bestmusicvideo', 'pillowtalk', 'joe', 'gt', 'lol', 'omg', \n",
    "    'paytforluckysun', 'michael'\n",
    "}\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "# Function to remove stop words from a tweet.\n",
    "def remove_stop_words(tweet):\n",
    "    # Tokenise the text.\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords.\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Return the cleaned text.\n",
    "    return ' '.join(tokens)\n",
    "    #return ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "\n",
    "# Apply remove_stop_words to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_stop_words)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d85e2-dd28-4b3b-babf-a02e11358424",
   "metadata": {},
   "source": [
    "## Handle Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b2e4e-fde0-45d8-a919-fd24fe5aa0b9",
   "metadata": {},
   "source": [
    "##### There was no missing data in this dataset at this point in the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3865368-f86a-4ea3-9a8b-2e5b71e1a68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values per column before data cleaning: \n",
      "id         0\n",
      "date       0\n",
      "tweet      0\n",
      "label      0\n",
      "weekday    0\n",
      "month      0\n",
      "year       0\n",
      "day        0\n",
      "hour       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the number of NaN values in each column.\n",
    "print(\"NaN values per column before data cleaning: \")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28d3ce9d-8066-492f-8754-f4bc0eaf2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to depression_cleaned.csv.\n",
    "df.to_csv('depression_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
