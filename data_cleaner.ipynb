{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "530d58ea-70c8-4539-882d-0bc232c1aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730431f-91e0-44da-9006-172dbb5705ce",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4e2e2a07-7c5b-46c7-a52c-b433e6916979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a dataframe.\n",
    "df = pd.read_csv(\"depression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ff3f4-6448-4db9-9ceb-fd4932957216",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc7aa8-8945-416e-8d7b-56461af3f2be",
   "metadata": {},
   "source": [
    "##### Only columns relevant for exploring the underlying themes of the tweets and how these themes correlate with depression are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "c4a4ec25-6e2c-4fc8-b2ed-7c35a697aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['user_id', 'followers', 'friends', 'favourites', 'statuses', 'retweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "71f00d86-d38f-4162-854c-19b13d29975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"depression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d85e2-dd28-4b3b-babf-a02e11358424",
   "metadata": {},
   "source": [
    "### Handle Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b2e4e-fde0-45d8-a919-fd24fe5aa0b9",
   "metadata": {},
   "source": [
    "##### There was no missing data in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e3865368-f86a-4ea3-9a8b-2e5b71e1a68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values per column before data cleaning: \n",
      "Unnamed: 0      0\n",
      "post_id         0\n",
      "post_created    0\n",
      "post_text       0\n",
      "user_id         0\n",
      "followers       0\n",
      "friends         0\n",
      "favourites      0\n",
      "statuses        0\n",
      "retweets        0\n",
      "label           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the number of NaN values in each column.\n",
    "print(\"NaN values per column before data cleaning: \")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f160-025c-440a-b424-aa36c2017bfa",
   "metadata": {},
   "source": [
    "### Handle Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cfd94-a6e4-41cc-98c0-474174efad68",
   "metadata": {},
   "source": [
    "##### There were no duplicated rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "dac9bec1-3520-493d-948f-68ca604fded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows before removal: 0\n"
     ]
    }
   ],
   "source": [
    " print(f\"Number of duplicated rows before removal: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac538e-5d7b-479b-a5bc-7512361292c2",
   "metadata": {},
   "source": [
    "### Convert Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d233e69-e88d-40fb-b62e-b964929eb762",
   "metadata": {},
   "source": [
    "#### Convert Tweet Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8175da56-2b6a-464d-aa3f-9029491097d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column from 'post_id' to 'id'.\n",
    "df = df.rename(columns={'post_id': 'id'})\n",
    "\n",
    "# Convert the identifiers to strings.\n",
    "df['id'] = df['id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8a6a1-4f20-449a-8571-ba0f675776eb",
   "metadata": {},
   "source": [
    "#### Convert Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "c5c25da2-bd9c-4e0c-896b-d53f169aef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column from 'post_created' to 'date'.\n",
    "df = df.rename(columns={'post_created': 'date'})\n",
    "\n",
    "# Convert the 'date' column to datetime format.\n",
    "df['date'] = pd.to_datetime(df['date'], format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "# Extract the relevant datetime components.\n",
    "df['weekday'] = df['date'].dt.strftime('%A')\n",
    "df['month'] = df['date'].dt.strftime('%B')\n",
    "df['year'] = df['date'].dt.year\n",
    "df['day'] = df['date'].dt.day\n",
    "df['hour'] = df['date'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17faaa4-b94f-4c18-ab68-2fabc4e13178",
   "metadata": {},
   "source": [
    "#### Convert Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "360a202f-1020-4026-978f-c3132b497f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT @SewHQ: #Retro bears make perfect gifts and...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rename the column from 'post_text' to 'tweet'.\n",
    "df = df.rename(columns={'post_text': 'tweet'})\n",
    "\n",
    "# Convert the tweets to strings.\n",
    "df['tweet'] = df['tweet'].astype(str)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa305b07-63aa-417b-8b9f-ac5e6a2defde",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26034d17-ed05-4b49-84ce-56f89bc82f5d",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "184b266c-12fd-401d-9213-4c729f0bc21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT @SewHQ: #Retro bears make perfect gifts and...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove URLs from a tweet using regex.\n",
    "def remove_urls(tweet):\n",
    "    return re.sub(r'http\\S+', '', tweet)\n",
    "\n",
    "# Apply remove_urls to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_urls)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe04352-43fb-49d9-b0e5-91f5a4e1933a",
   "metadata": {},
   "source": [
    "### Remove Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba9e30-240e-47dd-b2b7-61db3108b189",
   "metadata": {},
   "source": [
    "##### Remove @username mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f792a8ed-6761-45fe-9fe8-d17cb9db7ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT : #Retro bears make perfect gifts and are g...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove mentions from a tweet using regex.\n",
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "# Apply remove_mentions to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_mentions)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7254f4-4dc9-4751-b45a-c159d9146056",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fab08048-7d23-4136-bcb7-c138a3d8ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Its just over 2 years since I was diagnosed wi...\n",
      "1        Its Sunday I need a break so Im planning to sp...\n",
      "2        Awake but tired I need to sleep but my brain h...\n",
      "3        RT  Retro bears make perfect gifts and are gre...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                 A day without sunshine is like night\n",
      "19996    Borens Laws 1 When in charge ponder 2 When in ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor but they were never m...\n",
      "19999        Black holes are where God is dividing by zero\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove punctuation from a tweet.\n",
    "def remove_punctuation(tweet):\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Apply remove_punctuation to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_punctuation)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4375861-b1ec-4890-83b4-cb4b9382acf4",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "9949d1bd-b890-4946-811d-0c8b6ddbeb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Its 2 years since I diagnosed anxiety depressi...\n",
      "1        Its Sunday I need break Im planning spend litt...\n",
      "2                     Awake tired I need sleep brain ideas\n",
      "3        RT Retro bears make perfect gifts great beginn...\n",
      "4        It’s hard say whether packing lists making lif...\n",
      "                               ...                        \n",
      "19995                    A day without sunshine like night\n",
      "19996    Borens Laws 1 When charge ponder 2 When troubl...\n",
      "19997    The flow chart thoroughly oversold piece progr...\n",
      "19998                   Ships safe harbor never meant stay\n",
      "19999                        Black holes God dividing zero\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the stopwords from nltk.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from a tweet.\n",
    "def remove_stop_words(tweet):\n",
    "    return ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "\n",
    "# Apply remove_stop_words to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_stop_words)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b438c-c0e5-49e0-a97f-fe8091dcc9cc",
   "metadata": {},
   "source": [
    "### Remove Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a62dc257-e971-4f3a-ad6c-d6c8ed2b486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Its 2 years since I diagnosed anxiety depressi...\n",
      "1        Its Sunday I need break Im planning spend litt...\n",
      "2                     Awake tired I need sleep brain ideas\n",
      "3        RT Retro bears make perfect gifts great beginn...\n",
      "4        It’s hard say whether packing lists making lif...\n",
      "                               ...                        \n",
      "19995                    A day without sunshine like night\n",
      "19996    Borens Laws 1 When charge ponder 2 When troubl...\n",
      "19997    The flow chart thoroughly oversold piece progr...\n",
      "19998                   Ships safe harbor never meant stay\n",
      "19999                        Black holes God dividing zero\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove extra spaces from a tweet with regex.\n",
    "def remove_extra_spaces(tweet):\n",
    "    return re.sub(r'\\s+', ' ', tweet).strip()\n",
    "\n",
    "# Apply remove_extra_spaces to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(remove_extra_spaces)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9af8f-eb41-440a-924b-7dc0c0e76e7b",
   "metadata": {},
   "source": [
    "### Convert Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "59686850-59bd-4a5b-ae7b-ee3d2c09152e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        its 2 years since i diagnosed anxiety depressi...\n",
      "1        its sunday i need break im planning spend litt...\n",
      "2                     awake tired i need sleep brain ideas\n",
      "3        rt retro bears make perfect gifts great beginn...\n",
      "4        it’s hard say whether packing lists making lif...\n",
      "                               ...                        \n",
      "19995                    a day without sunshine like night\n",
      "19996    borens laws 1 when charge ponder 2 when troubl...\n",
      "19997    the flow chart thoroughly oversold piece progr...\n",
      "19998                   ships safe harbor never meant stay\n",
      "19999                        black holes god dividing zero\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'tweets' column to lowercase.\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9beb42-a68d-40e8-9a36-197e1eb2f46a",
   "metadata": {},
   "source": [
    "### Lemmatise Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "da0b2b0d-f456-4807-97b4-341279ad224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/melodyflavel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0a99a003-db7d-4009-800b-85b924c60a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise lemmatiser.\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74fb23d-ae69-4fa9-88c0-6b16e476a45b",
   "metadata": {},
   "source": [
    "#### Lemmatise Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "43d6eb03-5d24-4df2-b130-4ca6b361a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        it 2 year since i diagnosed anxiety depression...\n",
      "1        it sunday i need break im planning spend littl...\n",
      "2                      awake tired i need sleep brain idea\n",
      "3        rt retro bear make perfect gift great beginner...\n",
      "4        it ’ s hard say whether packing list making li...\n",
      "                               ...                        \n",
      "19995                    a day without sunshine like night\n",
      "19996    borens law 1 when charge ponder 2 when trouble...\n",
      "19997    the flow chart thoroughly oversold piece progr...\n",
      "19998                    ship safe harbor never meant stay\n",
      "19999                         black hole god dividing zero\n",
      "Name: tweet, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function for lemmatising nouns in a tweet.\n",
    "def lemmatise_nouns(tweet):\n",
    "    # Tokenise the tweet into individual words.\n",
    "    words = word_tokenize(tweet)\n",
    "    # Lemmatise each noun in the tweet.\n",
    "    lemmatised_words = [lemmatiser.lemmatize(word) for word in words]\n",
    "    # Join the lemmatised words back into a string.\n",
    "    return \" \".join(lemmatised_words)\n",
    "\n",
    "# Apply lemmatise_nouns to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(lemmatise_nouns)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e289486-98f1-4dc6-bed6-1183e2b0eb4b",
   "metadata": {},
   "source": [
    "#### Lemmatise Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041adb0e-91c3-4648-9a03-c862599b61b7",
   "metadata": {},
   "source": [
    "The WordNetLemmatizer assumes all words are nouns. To get more accurate lemmatisation, such that verbs and adjectives are also lemmatised, part-of-speech (POS) tags can be used to specify the correct word category, allowing the lemmatiser to reduce words to their base forms based on their actual usage in the sentence. For example, 'running' is lemmatised to 'run', and 'better' to 'good'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268d446-62e4-446a-b2f8-55cc3ec797cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatising words in a tweet with POS.\n",
    "def lemmatise_words_with_pos(tweet):\n",
    "    # Tokenise the tweet into individual words.\n",
    "    words = word_tokenize(tweet)\n",
    "    # Get POS tags for each word.\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    # Map POS tags to WordNet POS tags.\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    # Lemmatise based on POS tags.\n",
    "    lemmatised_words = [lemmatiser.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "    # Join the lemmatised words back into a string.\n",
    "    return \" \".join(lemmatised_words)\n",
    "\n",
    "# Apply lemmatise_words_with_pos to 'tweet' column.\n",
    "df['tweet'] = df['tweet'].apply(lemmatise_words_with_pos)\n",
    "\n",
    "print(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf91fb5-7aa2-41ab-af39-e9ff5606df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'pandas version: {pd.__version__}')\n",
    "print(f'nltk version: {nltk.__version__}')\n",
    "#print(f'stopwords version: {stopwords.__name__}')  # Stopwords is part of nltk, version is same as nltk\n",
    "print(f're version: {re.__version__}')  # 're' doesn't have a version number, so skip this\n",
    "print(f'string version: {string.__name__}')  # 'string' doesn't have a version number, so skip this\n",
    "print(f'WordNetLemmatizer version: {nltk.stem.WordNetLemmatizer.__module__}')  # No version for lemmatizer specifically, same as nltk\n",
    "#print(f'word_tokenize version: {nltk.tokenize.__version__}')\n",
    "#print(f'wordnet version: {wordnet.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3ce9d-8066-492f-8754-f4bc0eaf2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to depression_cleaned.csv.\n",
    "df.to_csv('depression_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210752b-9368-409e-acf7-05b315d9a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
